# -*- coding: utf-8 -*-
"""SimpleLinearRegression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-p7_c4DrEi5AwKR5-kvTfzwh_M6Txp6f
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import pylab as pl
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from sklearn import linear_model
from sklearn.metrics import r2_score, PredictionErrorDisplay

!wget -O FuelConsumption.csv https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%202/data/FuelConsumptionCo2.csv

!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%202/data/FuelConsumptionCo2.csv -o FuelConsumptionCo2.csv

df = pd.read_csv("FuelConsumption.csv")

df.head()

df.columns.tolist(), len(df.columns.tolist())

df.describe()

#no missing data all columns elements same

cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]

cdf.head()

vis = cdf[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]
vis.hist()
plt.show()

"""CO2EMISSIONS vs FUELCONSUMPTION_COMB  """

plt.scatter(cdf.FUELCONSUMPTION_COMB, cdf.CO2EMISSIONS, color = 'blue')
plt.xlabel('FUELCONSUMPTION_COMB')
plt.ylabel('CO2EMISSIONS')
plt.show()

"""CO2EMISSIONS vs ENGINESIZE

"""

plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS, color = 'green')
plt.xlabel('ENGINESIZE')
plt.ylabel('CO2EMISSIONS')
plt.show()

"""CO2EMISSIONS vs CYLINERS"""

plt.scatter(cdf.CYLINDERS, cdf.CO2EMISSIONS, color = 'green')
plt.xlabel('CYLINDERS')
plt.ylabel('CO2EMISSIONS')
plt.show()

"""Creating train and test dataset"""

msk = np.random.rand(len(df)) <0.8
train = cdf[msk]
test = cdf[~msk]

len(train), len(test), len(cdf)

"""Train data distribution"""

plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS, color = 'Blue', label = 'Train')
plt.scatter(test.ENGINESIZE, test.CO2EMISSIONS, color = 'grey', label = 'Test', alpha = 0.5)
plt.xlabel('FUELCONSUMPTION_COMB')
plt.ylabel('CO2EMISSIONS')
plt.legend()
plt.show()

"""Modelling - Engine size vs CO2 emmission"""

reg = linear_model.LinearRegression()
train_x = np.asanyarray(train[['ENGINESIZE']])
train_y = np.asanyarray(train[['CO2EMISSIONS']])

reg.fit(train_x, train_y)
print('Coefficent: ', reg.coef_)
print('Intercept: ',reg.intercept_)

"""Plot the regression line on top of the train data points"""

plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS, color = 'Blue', label = 'Train')
plt.plot(train_x, train_x*reg.coef_[0][0]+reg.intercept_[0], '-r', label = 'Fitted')
plt.xlabel('ENGINESIZE')
plt.ylabel('CO2EMISSIONS')
plt.legend()
plt.show()

"""Evalution of the fitted model"""

#prepare the test data and predict
test_x =np.asanyarray(test[['ENGINESIZE']])
test_y = np.asanyarray(test[['CO2EMISSIONS']])
test_yhat = reg.predict(test_x)

#compare the prediction vs actual
print("MAE: %.2f"% np.mean(np.absolute(test_yhat - test_y)))
print("Residual sum of sqaures(MSE): %.2f"% np.mean((test_yhat - test_y)**2))
print("R-sqd: %.2f"% r2_score(test_yhat, test_y))

"""Modelling Fuel consumption vs CO2 emmission"""

def linear_reg(cdf, feature, target):
  msk = np.random.rand(len(df)) <0.8
  train = cdf[msk]
  test = cdf[~msk]


  train_x = np.asanyarray(train[[feature]])
  train_y = np.asanyarray(train[[target]])

  test_x = np.asanyarray(test[[feature]])
  test_y = np.asanyarray(test[[target]])

  reg = linear_model.LinearRegression()
  reg.fit(train_x, train_y)

  test_yhat = reg.predict(test_x)

  plt.scatter(train_x, train_y, color = 'Blue', label = 'Train')
  plt.plot(train_x, train_x*reg.coef_[0][0]+reg.intercept_[0], '-r', label = 'Fitted')
  plt.xlabel(f'{feature}')
  plt.ylabel(f'{target}')
  plt.legend()
  plt.show()

  print(f'{feature} vs {target} - ',"MAE : %.2f"% np.mean(np.absolute(test_yhat - test_y)))
  print(f'{feature} vs {target} - ',"Mean Squared Error(MSE): %.2f"% np.mean((test_yhat - test_y)**2))
  print(f'{feature} vs {target} - ',"R-sqd: %.2f"% r2_score(test_yhat, test_y))

linear_reg(cdf, 'FUELCONSUMPTION_COMB', 'CO2EMISSIONS')

#the result will change each time we rundue to the random choice of the mask values. This is the reason we use K-Fold cross validation
linear_reg(cdf, 'ENGINESIZE', 'CO2EMISSIONS')

"""#comparison
1. ENGINESIZE vs CO2EMISSIONS has higher MAE than FUELCONSUMPTION_COMB vs
CO2EMISSIONS
2. Fuel consumption has higher R-sqd valu than engine size, meaning prediction CO2 emmsion based on the fuel consumption is more relaible than predicting using engine size


"""



"""##check the assumption of SLR
Ref:: https://online.stat.psu.edu/stat500/lesson/9/9.2/9.2.3

## 1. Linearity:
The relationship between feature and target must be linear.
Check this assumption by examining a scatterplot of x and y.

###2. Independence of errors:
There is not a relationship between the residuals and the Y variable; in other words, Y is independent of errors.

  Check this assumption by examining a scatterplot of “residuals versus fits”; the correlation should be approximately 0. In other words, there should not look like there is a relationship.
"""

def residual_vs_fit(cdf, feature, target):
  msk = np.random.rand(len(df)) <0.8
  train = cdf[msk]
  test = cdf[~msk]


  train_x = np.asanyarray(train[[feature]])
  train_y = np.asanyarray(train[[target]])

  test_x = np.asanyarray(test[[feature]])
  test_y = np.asanyarray(test[[target]])

  reg = linear_model.LinearRegression()
  reg.fit(train_x, train_y)

  test_yhat = reg.predict(test_x)

  display = PredictionErrorDisplay(y_true=test_y, y_pred = test_yhat)
  display.plot()

  plt.show()

residual_vs_fit(cdf, 'ENGINESIZE', 'CO2EMISSIONS')

"""##3.Normality of errors:
The residuals must be approximately normally distributed.

Check this assumption by examining a normal probability plot; the observations should be near the line. You can also examine a histogram of the residuals; it should be approximately normally distributed.
"""

def normality_errors(cdf, feature, target):
  msk = np.random.rand(len(df)) <0.8
  train = cdf[msk]
  test = cdf[~msk]


  train_x = np.asanyarray(train[[feature]])
  train_y = np.asanyarray(train[[target]])

  test_x = np.asanyarray(test[[feature]])
  test_y = np.asanyarray(test[[target]])

  reg = linear_model.LinearRegression()
  reg.fit(train_x, train_y)

  test_yhat = reg.predict(test_x)

  residuals = test_y - test_yhat

  sns.displot(residuals, kde = True)
  # plt.hist(residuals)
  # plt.show()

#cheack the residauls are normally distributed?
normality_errors(cdf, 'ENGINESIZE', 'CO2EMISSIONS')

"""##4.Equal variances:
The variance of the residuals is the same for all values of X

Check this assumption by examining the scatterplot of “residuals versus fits”; the variance of the residuals should be the same across all values of the x-axis. If the plot shows a pattern (e.g., bowtie or megaphone shape), then variances are not consistent, and this assumption has not been met.
"""

def residual_variance(cdf, feature, target):
  msk = np.random.rand(len(df)) <0.8
  train = cdf[msk]
  test = cdf[~msk]


  train_x = np.asanyarray(train[[feature]])
  train_y = np.asanyarray(train[[target]])

  test_x = np.asanyarray(test[[feature]])
  test_y = np.asanyarray(test[[target]])

  reg = linear_model.LinearRegression()
  reg.fit(train_x, train_y)

  test_yhat = reg.predict(test_x)

  residuals = test_y - test_yhat
  max_resid, min_resid = max(residuals), min(residuals)

  display = PredictionErrorDisplay(y_true=test_y, y_pred = test_yhat)
  display.plot()

  plt.plot(np.arange(min(test_yhat), max(test_yhat)), max_resid*np.ones(len(np.arange(min(test_yhat), max(test_yhat)))))
  plt.plot(np.arange(min(test_yhat), max(test_yhat)), min_resid*np.ones(len(np.arange(min(test_yhat), max(test_yhat)))))
  plt.show()

residual_variance(cdf, 'ENGINESIZE', 'CO2EMISSIONS')

